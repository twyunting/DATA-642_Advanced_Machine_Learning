{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab10_Yunting.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1QNpszfbrvTvOcEfAZRNa-HWRuKF3Vq9B","authorship_tag":"ABX9TyMepj03EFJgasNOKJ5Od3lO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U2G9GgIsQyZo"},"source":["# Lab08 - Neural Networks and Deep Learning\n","Author: [Yunting Chiu](https://www.linkedin.com/in/yuntingchiu/) "]},{"cell_type":"markdown","metadata":{"id":"QqGwczwKQ_ib"},"source":["# Install the required packages"]},{"cell_type":"code","metadata":{"id":"NQIvcuZnWUe5","executionInfo":{"status":"ok","timestamp":1637381770313,"user_tz":300,"elapsed":161,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import initializers\n","\n","# Let Keras know that we are using tensorflow as our backend engine\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQMnIvrvWWNE"},"source":["＃Exercise 1\n","This exercise focuses on the original GAN, which is trained on the MNIST database to learn\n","to generate artificial hand-written characters. To implement and train the network, use the\n","TensorFlow framework. You are encouraged to use the Keras high-level API for simplicity.\n","\n","(a) Load the data set of 60000 to be used for training. Normalize image values so that all\n","values are in the $[−1, 1]$ interval. In Keras, you may use the functions provided by the\n","`tensorflow.keras.datasets.mnist` module."]},{"cell_type":"code","metadata":{"id":"-MmqopatXmwa","executionInfo":{"status":"ok","timestamp":1637382732340,"user_tz":300,"elapsed":321,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["# to make sure we have the same results for each time\n","np.random.seed(1234)"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fy-u2mBmT2mu","executionInfo":{"status":"ok","timestamp":1637382733531,"user_tz":300,"elapsed":522,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}},"outputId":"28702b65-100d-49ed-db83-6e67b3ddb9b4"},"source":["# load the data\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","# normalize the inputs to be in the range[-1, 1]\n","x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n","# convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have 784 columns pre row\n","x_train = x_train.reshape(60000, 784)\n","print(x_train.shape)\n","print(x_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)"],"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n","(10000, 28, 28)\n","(60000,)\n","(10000,)\n"]}]},{"cell_type":"code","metadata":{"id":"VPmXHevrmNk9","executionInfo":{"status":"ok","timestamp":1637382734041,"user_tz":300,"elapsed":2,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def load_MINST():\n","  # load the data\n","  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","  # normalize our inputs to be in the range[-1, 1]\n","  x_train = (x_train.astype(np.float32) - 127.5)/127.5\n","  # convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have\n","  # 784 columns per row\n","  x_train = x_train.reshape(60000, 784)\n","  return (x_train, y_train, x_test, y_test)"],"execution_count":70,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lL6ZEDzDBaqY"},"source":["(b) The generator is fed to its input with a noise vector and outputs an image. The dimension\n","of the input noise vector is 100, and its elements are i.i.d. sampled from a normal distribution of zero mean and unit variance. The generator consists of three fully connected\n","layers with 256, 512, and 1024 neurons, respectively. The activation function used for the\n","neurons in the hidden layers is the leaky ReLU with parameter α = 0.2. The output layer\n","comprises 784 nodes and the tanh is employed as the respective activation function.\n","\n","- Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models."]},{"cell_type":"code","metadata":{"id":"IaJYUoDdUkcB","executionInfo":{"status":"ok","timestamp":1637382736232,"user_tz":300,"elapsed":151,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["# The dimension of our random noise vector.\n","random_dim = 100\n","\n","# We will use the Adam optimizer\n","def get_optimizer():\n","  return Adam(lr = 0.0002, beta_1 = 0.5)"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"ae1XxbpPUvpJ","executionInfo":{"status":"ok","timestamp":1637382826133,"user_tz":300,"elapsed":139,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def get_generator(optimizer):\n","  generator = Sequential()\n","  # 256 neurons\n","  generator.add(Dense(256, input_dim=random_dim,kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n","  generator.add(LeakyReLU(0.2))\n","\n","  # 512 neurons\n","  generator.add(Dense(512))\n","  generator.add(LeakyReLU(0.2))\n","\n","  # 1024 neurons\n","  generator.add(Dense(1024))\n","  generator.add(LeakyReLU(0.2))\n","\n","  generator.add(Dense(784, activation='tanh'))\n","  generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n","  return generator"],"execution_count":77,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7SoSEWxOB6TK"},"source":["(c) The discriminator takes as input a 1 × 784 vector, corresponding to the vectorized form\n","of the 28 × 28 MNIST images. The discriminator consists of three fully connected layers\n","with 1024, 512, and 256 neurons, respectively. The leaky ReLU activation function is also\n","employed, with $\\alpha$ = 0.2. During training, use the dropout regularization method, with\n","probability of discarding nodes equal to 0.3. The output layer consist of a single node\n","with a sigmoid activation function."]},{"cell_type":"code","metadata":{"id":"uCpG87xuUTou","executionInfo":{"status":"ok","timestamp":1637382738268,"user_tz":300,"elapsed":150,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def get_discriminator(optimizer):\n","  discriminator = Sequential()\n","  discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n","  discriminator.add(LeakyReLU(0.2))\n","  # use the dropout regularization method, with probability of discarding nodes equal to 0.3\n","  discriminator.add(Dropout(0.3))\n","\n","  discriminator.add(Dense(512))\n","  discriminator.add(LeakyReLU(0.2))\n","  discriminator.add(Dropout(0.3))\n","\n","  discriminator.add(Dense(256))\n","  discriminator.add(LeakyReLU(0.2))\n","  discriminator.add(Dropout(0.3))\n","\n","  discriminator.add(Dense(1, activation='sigmoid'))\n","  discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n","  return discriminator"],"execution_count":73,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yNEod2kxB9Mv"},"source":["(d) To train the implemented network, use the two-class (binary) cross-entropy loss function.\n","Adopt the Adam minimizer with step size (learning rate) equal to $2 ×10^{−3}$\n",", $\\beta1$ = 0.5, and\n","$\\beta2$ = 0.999 as parameters for the optimizer. The recommended batch size is 100. Train\n","the network for 400 epochs as follows. For each training loop, (a) generate a random set\n","of input noise and images, (b) generate fake images via the generator, (c) train only the\n","discriminator, and (d) then train only the generator, according to Algorithm 18.5. Play\n","with the number of iterations, associated with the discriminator training."]},{"cell_type":"code","metadata":{"id":"EvyKnSHKg_AA","executionInfo":{"status":"ok","timestamp":1637382739464,"user_tz":300,"elapsed":153,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def get_gan_network(discriminator, random_dim, generator, optimizer):\n","  # We initially set trainable to False since we only want to train either the generator or discriminator at a time\n","  discriminator.trainable = False\n","\n","  # gan input (noise) will be 100-dimensional vectors\n","  gan_input = Input(shape=(random_dim,))\n","\n","  # the output of the generator (an image)\n","  x = generator(gan_input)\n","  # get the output of the discriminator (probability if the image is real or not)\n","  gan_output = discriminator(x)\n","  gan = Model(inputs=gan_input, outputs=gan_output)\n","  gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n","  return gan"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbfyW-cvh3Mf","executionInfo":{"status":"ok","timestamp":1637382740280,"user_tz":300,"elapsed":3,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def plot_generated_images(epoch, generator, examples=100, dim=(10, 10), figsize=(10, 10)):\n","  noise = np.random.normal(0, 1, size=[examples, random_dim])\n","  generated_images = generator.predict(noise)\n","  generated_images = generated_images.reshape(examples, 28, 28)\n","  plt.figure(figsize=figsize)\n","  for i in range(generated_images.shape[0]):\n","    plt.subplot(dim[0], dim[1], i+1)\n","    plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n","    plt.axis('off')\n","    plt.tight_layout()\n","  %cd /content/drive/MyDrive/American_University/2021_Fall/DATA-642-001_Advanced Machine Learning/GitHub/Labs/10/submit\n","  plt.savefig('gan_generated_image_epoch_%d.png' % epoch)"],"execution_count":75,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AdZwJBNbB_3E"},"source":["(e) During training and every 20 epochs, visualize the generated images created by the generator and comment on the evolution of the learning process."]},{"cell_type":"code","metadata":{"id":"z8gs-qQgiHey","executionInfo":{"status":"ok","timestamp":1637382844243,"user_tz":300,"elapsed":160,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def train(epochs=1, batch_size=128):\n","  # Get the training and testing data\n","  x_train, y_train, x_test, y_test = load_MINST()\n","  # Split the training data into batches of size 128\n","  batch_count = x_train.shape[0] / batch_size\n","\n","  # Build the GAN netowrk\n","  adam = get_optimizer()\n","  generator = get_generator(adam)\n","  discriminator = get_discriminator(adam)\n","  gan = get_gan_network(discriminator, random_dim, generator, adam)\n","  for e in range(1, epochs+1):\n","    print('-'*15, 'Epoch %d' % e, '-'*15)\n","    for _ in tqdm(range(0, int(batch_count))):\n","      # Get a random set of input noise and images\n","      noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n","      image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n","      # Generate fake MNIST images\n","      generated_images = generator.predict(noise)\n","      X = np.concatenate([image_batch, generated_images])\n","      # Labels for generated and real data\n","      y_dis = np.zeros(2*batch_size)\n","      # One-sided label smoothing\n","      y_dis[:batch_size] = 0.9\n","      3\n","      # Train discriminator\n","      discriminator.trainable = True\n","      discriminator.train_on_batch(X, y_dis)\n","      # Train generat or\n","      noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n","      y_gen = np.ones(batch_size)\n","      discriminator.trainable = False\n","      gan.train_on_batch(noise, y_gen)\n","    if e == 1 or e % 20 == 0:\n","      plot_generated_images(e, generator)"],"execution_count":79,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ve3ZZZnCB7j"},"source":["(f) Play with all the various parameters that have been suggested above and see the effect on\n","the training."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eN3Xmwvqi52t","outputId":"cc2a52cc-bae1-43d3-d429-38e4d9f360dd"},"source":["if __name__ == '__main__':\n","  train(400, 128)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 1 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:21<00:00,  5.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/American_University/2021_Fall/DATA-642-001_Advanced Machine Learning/GitHub/Labs/10/submit\n","--------------- Epoch 2 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:19<00:00,  5.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 3 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 4 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 5 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 6 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:19<00:00,  5.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 7 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 8 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 9 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 10 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:19<00:00,  5.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 11 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 12 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:21<00:00,  5.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 13 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 14 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 15 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:19<00:00,  5.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 16 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 17 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 18 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 19 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:21<00:00,  5.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 20 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:22<00:00,  5.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/American_University/2021_Fall/DATA-642-001_Advanced Machine Learning/GitHub/Labs/10/submit\n","--------------- Epoch 21 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 22 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 23 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 24 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:19<00:00,  5.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 25 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 26 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 27 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:19<00:00,  5.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 28 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:20<00:00,  5.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 29 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 468/468 [01:19<00:00,  5.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 30 ---------------\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▋       | 124/468 [00:21<00:57,  5.95it/s]"]}]},{"cell_type":"markdown","metadata":{"id":"IgkGjlQtotNQ"},"source":["We must use activation functions such as ReLu, sigmoid and tanh in order to add a non-linear property to the neural network. In this way, the network can model more complex relationships and patterns in the data."]},{"cell_type":"markdown","metadata":{"id":"5IEGQ9Tv-xue"},"source":["# References\n","- https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.\n","- https://towardsdatascience.com/activation-functions-in-deep-neural-networks-aae2a598f211\n"]},{"cell_type":"markdown","metadata":{"id":"YyjMxsO7ch3F"},"source":["# Testing Zone"]},{"cell_type":"markdown","metadata":{"id":"qWZoyFcw971A"},"source":["# Output"]},{"cell_type":"code","metadata":{"id":"Zh6y-Cbx986W","executionInfo":{"status":"aborted","timestamp":1637381771543,"user_tz":300,"elapsed":5,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["# should access the Google Drive files before running the chunk\n","%%capture\n","!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic \n","!jupyter nbconvert --to pdf \"/content/drive/MyDrive/American_University/2021_Fall/DATA-642-001_Advanced Machine Learning/GitHub/Labs/10/submit/Lab10_Yunting.ipynb\""],"execution_count":null,"outputs":[]}]}