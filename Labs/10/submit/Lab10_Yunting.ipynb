{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab10_Yunting.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1QNpszfbrvTvOcEfAZRNa-HWRuKF3Vq9B","authorship_tag":"ABX9TyM3rrWUXL5x8nCOn72lbj2N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U2G9GgIsQyZo"},"source":["# Lab08 - Neural Networks and Deep Learning\n","Author: [Yunting Chiu](https://www.linkedin.com/in/yuntingchiu/) adapted from Dr. Zois Boukouvalas"]},{"cell_type":"markdown","metadata":{"id":"QqGwczwKQ_ib"},"source":["# Install the required packages"]},{"cell_type":"code","metadata":{"id":"NQIvcuZnWUe5","executionInfo":{"status":"ok","timestamp":1637460574229,"user_tz":300,"elapsed":2808,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import initializers\n","\n","# Let Keras know that we are using tensorflow as our backend engine\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQMnIvrvWWNE"},"source":["＃Exercise 1\n","This exercise focuses on the original GAN, which is trained on the MNIST database to learn\n","to generate artificial hand-written characters. To implement and train the network, use the\n","TensorFlow framework. You are encouraged to use the Keras high-level API for simplicity.\n","\n","(a) Load the data set of 60000 to be used for training. Normalize image values so that all\n","values are in the $[−1, 1]$ interval. In Keras, you may use the functions provided by the\n","`tensorflow.keras.datasets.mnist` module."]},{"cell_type":"code","metadata":{"id":"-MmqopatXmwa","executionInfo":{"status":"ok","timestamp":1637460574230,"user_tz":300,"elapsed":7,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["# to make sure we have the same results for each time\n","np.random.seed(1234)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fy-u2mBmT2mu","executionInfo":{"status":"ok","timestamp":1637460574959,"user_tz":300,"elapsed":735,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}},"outputId":"dc26f174-ee5f-455e-a4b8-6c6cc51372ef"},"source":["# load the data\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","# normalize the inputs to be in the range[-1, 1]\n","x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n","# convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have 784 columns pre row\n","x_train = x_train.reshape(60000, 784)\n","print(x_train.shape)\n","print(x_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","(60000, 784)\n","(10000, 28, 28)\n","(60000,)\n","(10000,)\n"]}]},{"cell_type":"code","metadata":{"id":"VPmXHevrmNk9","executionInfo":{"status":"ok","timestamp":1637460576392,"user_tz":300,"elapsed":6,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def load_MINST():\n","  # load the data\n","  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","  # normalize our inputs to be in the range[-1, 1]\n","  x_train = (x_train.astype(np.float32) - 127.5)/127.5\n","  # convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have\n","  # 784 columns per row\n","  x_train = x_train.reshape(60000, 784)\n","  return (x_train, y_train, x_test, y_test)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lL6ZEDzDBaqY"},"source":["(b) The generator is fed to its input with a noise vector and outputs an image. The dimension\n","of the input noise vector is 100, and its elements are i.i.d. sampled from a normal distribution of zero mean and unit variance. The generator consists of three fully connected\n","layers with 256, 512, and 1024 neurons, respectively. The activation function used for the\n","neurons in the hidden layers is the leaky ReLU with parameter α = 0.2. The output layer\n","comprises 784 nodes and the tanh is employed as the respective activation function.\n","\n","- Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models."]},{"cell_type":"code","metadata":{"id":"IaJYUoDdUkcB","executionInfo":{"status":"ok","timestamp":1637460577412,"user_tz":300,"elapsed":135,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["# The dimension of our random noise vector.\n","random_dim = 100\n","\n","# We will use the Adam optimizer\n","def get_optimizer():\n","  return Adam(lr = 0.0002, beta_1 = 0.5)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ae1XxbpPUvpJ","executionInfo":{"status":"ok","timestamp":1637460578095,"user_tz":300,"elapsed":2,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def get_generator(optimizer):\n","  generator = Sequential()\n","  # 256 neurons\n","  generator.add(Dense(256, input_dim=random_dim,kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n","  generator.add(LeakyReLU(0.2))\n","\n","  # 512 neurons\n","  generator.add(Dense(512))\n","  generator.add(LeakyReLU(0.2))\n","\n","  # 1024 neurons\n","  generator.add(Dense(1024))\n","  generator.add(LeakyReLU(0.2))\n","\n","  generator.add(Dense(784, activation='tanh'))\n","  generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n","  return generator"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7SoSEWxOB6TK"},"source":["(c) The discriminator takes as input a 1 × 784 vector, corresponding to the vectorized form\n","of the 28 × 28 MNIST images. The discriminator consists of three fully connected layers\n","with 1024, 512, and 256 neurons, respectively. The leaky ReLU activation function is also\n","employed, with $\\alpha$ = 0.2. During training, use the dropout regularization method, with\n","probability of discarding nodes equal to 0.3. The output layer consist of a single node\n","with a sigmoid activation function."]},{"cell_type":"code","metadata":{"id":"uCpG87xuUTou","executionInfo":{"status":"ok","timestamp":1637460579396,"user_tz":300,"elapsed":2,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def get_discriminator(optimizer):\n","  discriminator = Sequential()\n","  discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n","  discriminator.add(LeakyReLU(0.2))\n","  # use the dropout regularization method, with probability of discarding nodes equal to 0.3\n","  discriminator.add(Dropout(0.3))\n","\n","  discriminator.add(Dense(512))\n","  discriminator.add(LeakyReLU(0.2))\n","  discriminator.add(Dropout(0.3))\n","\n","  discriminator.add(Dense(256))\n","  discriminator.add(LeakyReLU(0.2))\n","  discriminator.add(Dropout(0.3))\n","\n","  discriminator.add(Dense(1, activation='sigmoid'))\n","  discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n","  return discriminator"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yNEod2kxB9Mv"},"source":["(d) To train the implemented network, use the two-class (binary) cross-entropy loss function.\n","Adopt the Adam minimizer with step size (learning rate) equal to $2 ×10^{−3}$\n",", $\\beta1$ = 0.5, and\n","$\\beta2$ = 0.999 as parameters for the optimizer. The recommended batch size is 100. Train\n","the network for 400 epochs as follows. For each training loop, (a) generate a random set\n","of input noise and images, (b) generate fake images via the generator, (c) train only the\n","discriminator, and (d) then train only the generator, according to Algorithm 18.5. Play\n","with the number of iterations, associated with the discriminator training."]},{"cell_type":"code","metadata":{"id":"EvyKnSHKg_AA","executionInfo":{"status":"ok","timestamp":1637460580893,"user_tz":300,"elapsed":139,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def get_gan_network(discriminator, random_dim, generator, optimizer):\n","  # We initially set trainable to False since we only want to train either the generator or discriminator at a time\n","  discriminator.trainable = False\n","\n","  # gan input (noise) will be 100-dimensional vectors\n","  gan_input = Input(shape=(random_dim,))\n","\n","  # the output of the generator (an image)\n","  x = generator(gan_input)\n","  # get the output of the discriminator (probability if the image is real or not)\n","  gan_output = discriminator(x)\n","  gan = Model(inputs=gan_input, outputs=gan_output)\n","  gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n","  return gan"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbfyW-cvh3Mf","executionInfo":{"status":"ok","timestamp":1637460581730,"user_tz":300,"elapsed":1,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def plot_generated_images(epoch, generator, examples=100, dim=(10, 10), figsize=(10, 10)):\n","  noise = np.random.normal(0, 1, size=[examples, random_dim])\n","  generated_images = generator.predict(noise)\n","  generated_images = generated_images.reshape(examples, 28, 28)\n","  plt.figure(figsize=figsize)\n","  for i in range(generated_images.shape[0]):\n","    plt.subplot(dim[0], dim[1], i+1)\n","    plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n","    plt.axis('off')\n","    plt.tight_layout()\n","  %cd /content/drive/MyDrive/American_University/2021_Fall/DATA-642-001_Advanced Machine Learning/GitHub/Labs/10/submit\n","  plt.savefig('gan_generated_image_epoch_%d.png' % epoch)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AdZwJBNbB_3E"},"source":["(e) During training and every 20 epochs, visualize the generated images created by the generator and comment on the evolution of the learning process."]},{"cell_type":"code","metadata":{"id":"z8gs-qQgiHey","executionInfo":{"status":"ok","timestamp":1637460583147,"user_tz":300,"elapsed":3,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["def train(epochs=1, batch_size=128):\n","  # Get the training and testing data\n","  x_train, y_train, x_test, y_test = load_MINST()\n","  # Split the training data into batches of size 128\n","  batch_count = x_train.shape[0] / batch_size\n","\n","  # Build the GAN netowrk\n","  adam = get_optimizer()\n","  generator = get_generator(adam)\n","  discriminator = get_discriminator(adam)\n","  gan = get_gan_network(discriminator, random_dim, generator, adam)\n","  for e in range(1, epochs+1):\n","    print('-'*15, 'Epoch %d' % e, '-'*15)\n","    for _ in tqdm(range(0, int(batch_count))):\n","      # Get a random set of input noise and images\n","      noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n","      image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n","      # Generate fake MNIST images\n","      generated_images = generator.predict(noise)\n","      X = np.concatenate([image_batch, generated_images])\n","      # Labels for generated and real data\n","      y_dis = np.zeros(2*batch_size)\n","      # One-sided label smoothing\n","      y_dis[:batch_size] = 0.9\n","      3\n","      # Train discriminator\n","      discriminator.trainable = True\n","      discriminator.train_on_batch(X, y_dis)\n","      # Train generat or\n","      noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n","      y_gen = np.ones(batch_size)\n","      discriminator.trainable = False\n","      gan.train_on_batch(noise, y_gen)\n","    if e == 1 or e % 20 == 0:\n","      plot_generated_images(e, generator)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ve3ZZZnCB7j"},"source":["(f) Play with all the various parameters that have been suggested above and see the effect on\n","the training.\n","\n","Iterations is the number of batches needed to complete one epoch. Note: The number of batches is equal to number of iterations for one epoch. We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch.\n","\n","We can see if the higher the epoch number, the clearer the image output. After epoch 180, We can easily distinguish digit numbers from our eyes.\n","- iteration= (total observation/batch_size). That is, 60000/128 = 400 iterations to complete 1 epoch."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1mKmBkbwpJ0RXl6DIFvoyO7gXr3KZoVTf"},"id":"eN3Xmwvqi52t","executionInfo":{"status":"ok","timestamp":1637450575643,"user_tz":300,"elapsed":33930858,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}},"outputId":"88cc2525-af7a-4403-cafa-b6df6faf7274"},"source":["if __name__ == '__main__':\n","  train(epochs=400, batch_size=128)"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"uBPdKyC7uEHe"},"source":["We have 60000 observations. We should try to increase the number of iteration in order to get better results."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8DsoIh1uF1A","executionInfo":{"status":"ok","timestamp":1637460587603,"user_tz":300,"elapsed":159,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}},"outputId":"a8d74d40-acce-4a49-9e74-c0dc34e11eb6"},"source":["x_train.shape[0]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["60000"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"Pl1EY1yUwCCu"},"source":["- iteration = (total observation/batch_size). That is, 60000/32 = 1875 iterations to complete 1 epoch."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ev7R4oGkfBa","outputId":"df3b2ee4-1953-454e-82f8-88038878cce6"},"source":["if __name__ == '__main__':\n","  train(epochs=200, batch_size=32)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 1 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:34<00:00,  8.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/American_University/2021_Fall/DATA-642-001_Advanced Machine Learning/GitHub/Labs/10/submit\n","--------------- Epoch 2 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:34<00:00,  8.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 3 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:36<00:00,  8.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 4 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:33<00:00,  8.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 5 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 6 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 7 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 8 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 9 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 10 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:34<00:00,  8.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 11 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:34<00:00,  8.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 12 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:31<00:00,  8.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 13 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 14 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:31<00:00,  8.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 15 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 16 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 17 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:31<00:00,  8.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 18 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 19 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 20 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/American_University/2021_Fall/DATA-642-001_Advanced Machine Learning/GitHub/Labs/10/submit\n","--------------- Epoch 21 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:30<00:00,  8.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 22 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:33<00:00,  8.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 23 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:31<00:00,  8.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 24 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:31<00:00,  8.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 25 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:33<00:00,  8.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 26 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 27 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:33<00:00,  8.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 28 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 29 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:31<00:00,  8.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 30 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:31<00:00,  8.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 31 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:32<00:00,  8.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 32 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:33<00:00,  8.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 33 ---------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1875/1875 [03:34<00:00,  8.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["--------------- Epoch 34 ---------------\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|███████▉  | 1493/1875 [02:52<00:42,  9.09it/s]"]}]},{"cell_type":"markdown","metadata":{"id":"IgkGjlQtotNQ"},"source":["We must use activation functions such as ReLu, sigmoid and tanh in order to add a non-linear property to the neural network. In this way, the network can model more complex relationships and patterns in the data."]},{"cell_type":"markdown","metadata":{"id":"5IEGQ9Tv-xue"},"source":["# References\n","- https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.\n","- https://towardsdatascience.com/activation-functions-in-deep-neural-networks-aae2a598f211\n","- https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/"]},{"cell_type":"markdown","metadata":{"id":"YyjMxsO7ch3F"},"source":["# Testing Zone"]},{"cell_type":"markdown","metadata":{"id":"qWZoyFcw971A"},"source":["# Output"]},{"cell_type":"code","metadata":{"id":"Zh6y-Cbx986W","executionInfo":{"status":"aborted","timestamp":1637381771543,"user_tz":300,"elapsed":5,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["# should access the Google Drive files before running the chunk\n","%%capture\n","!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic \n","!jupyter nbconvert --to pdf \"/content/drive/MyDrive/American_University/2021_Fall/DATA-642-001_Advanced Machine Learning/GitHub/Labs/10/submit/Lab10_Yunting.ipynb\""],"execution_count":null,"outputs":[]}]}