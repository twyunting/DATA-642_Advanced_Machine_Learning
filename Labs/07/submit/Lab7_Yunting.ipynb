{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab7_Yunting.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1qbtZOoy1wC7TOqutVJA5lXlFyMBtqn0f","authorship_tag":"ABX9TyPQmNe2ViUij3DvG7DeKbcb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U2G9GgIsQyZo"},"source":["# Lab07 - Sparsity Aware Learning\n","Author: [Yunting Chiu](https://www.linkedin.com/in/yuntingchiu/) adapted from Dr. Zois Boukouvalas\n"]},{"cell_type":"markdown","metadata":{"id":"QqGwczwKQ_ib"},"source":["# Install the required packages"]},{"cell_type":"code","metadata":{"id":"-bjTyFkkS7jP","executionInfo":{"status":"ok","timestamp":1634746781879,"user_tz":240,"elapsed":106,"user":{"displayName":"Yunting Chiu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_QuRP-FvpZwye5zw3rmJmceg28bQqANBEfLr_13E=s64","userId":"09054757205289220354"}}},"source":["import matplotlib.pyplot as plt\n","import math\n","import numpy as np\n","import sys\n","import os"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nOprePutS9Hz"},"source":["# Exercise 1\n","\n","For this exercise, the performance of the SVM is tested in the context of a two-class two- dimensional classification task. The data set comprises $N$ = 150 points uniformly distributed in the region $[−5, 5] \\times [−5, 5]$. For each point $x_n = [x_n,1, x_n,2]^T$, we compute \n","\n","<br/> \n","$$\n","y_n = 0.05x^3_{n,1} + 0.05x^2_{n,1} + 0.05x_{n,1} + 0.05 + \\eta,\n","$$\n","<br/>\n","where $\\eta$ stands for zero mean Gaussian noise of variance $\\sigma^2_{\\eta}$ = 4. The point is assigned to either of the two classes, depending on the value of the noise as well as its position with respect to the graph of the function\n","\n","<br/>\n","$$\n","f (x) = 0.05x^3 + 0.05x^2 + 0.05x + 0.05\n","$$\n","\n","in the two-dimensional space. That is, if $x_{n,2} \\geq y_n$, the point is assigned to class $w1$; otherwise, it is assigned to class $w2$."]},{"cell_type":"code","metadata":{"id":"9XFExjHNfh_1"},"source":["def kappa(x, y, kernel_type, kernel_params): value = None\n","  if kernel_type == 'gaus':\n","    sigma = kernel_params[0]\n","    N = x.shape[0]\n","    norm = np.sum((x-y)**2)\n","    value = np.exp(-norm/(sigma**2))\n","  elif kernel_type == 'gaus_c':\n","    sigma = kernel_params[0]\n","    N = len(x)\n","    exponent = sum( (x-y.conj())**2 )\n","    # value = 2*(math.exp( -np.real(exponent)/(sigma**2) )) \n","    value = 2*np.real(np.exp(-exponent/(sigma**2)))\n","  elif kernel_type == 'linear': \n","    value = 0\n","    N = len(x)\n","    for i in range(0, N):\n","      value = value + x[i]*y[i].conj() \n","  elif kernel_type == 'poly':\n","    d = kernel_params[0]\n","    value = (1 + np.dot(x, y.conj().transpose()))**d\n","  # value = ( (1 + x*y.transpose())/( math.sqrt(np.real(x*x.transpose())* np.real(y*y.transpose()) ) ) )**d; \n","  elif kernel_type == 'poly_c':\n","    d = kernel_params[0]\n","    value = 2*np.real((1 + np.dot(x, y.conj().transpose()))**d)\n","  # value = 2*np.real( ( (1 + x*y.transpose())/( math.sqrt(np.real(x*x.transpose()) * np.real(y*y.transpose()) ) ) )**d ) \n","  return value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-oG9Dm_ngghd"},"source":["global a1_g, a2_g, b_g, u_g, KKT_g, NB_g, a1i_new, a1j_new, a2i_new, a2j_new\n","\"\"\"\n","# SMO algorithm for classification.\n","# The Algorithm computes the parameters a_k, of the expansion of the solution w = Sum a_n*K(. , x_n) and the parameter b.\n","# -------------------------------------------------------------------\n","# input variables\n","# x:\n","# y:\n","# C:\n","# -----------------------------------------------------------------\n","# output variables\n","# a  :\n","# b  :\n","\"\"\"\n","global a1_g, a2_g, b_g, u_g, KKT_g, NB_g, a1i_new, a1j_new, a2i_new, a2j_new\n","\n","def SMO_classification(x, y, C, epsilon, kernel_type, kernel_params):\n","  global a_g, b_g, u_g, KKT_g, NB_g tol = 0.001\n","  [M, N] = x.shape\n","  Kernel_matrix = np.zeros(shape=(M, M))\n","  if kernel_type == 'gaus':\n","  par = kernel_params\n","  norms = np.zeros(shape=(M, M)) for i in range(0, M):\n","  T = x - x[i, :] # bsxfun(@minus,x,x(i,:)) norms[i, :] = np.sum(T ** 2, axis=1)\n","  Kernel_matrix[:, :] = np.exp(-norms / (par ** 2)) else:\n","  for i in range(0, M): for j in range(0, M):\n","  a M x N matrix. It contains the input vectors x_i (each one in\n","  every row.\n","  a vector of size M. The class of each input vector\n","  the trade-off parameter of the SVM model\n","  a vector of size m. These are the support vectors\n","  a real number. This is the offset of the support vector\n","  expansion.\n","  2\n","\n","  Kernel_matrix[i, j] = kappa(x[i, :], x[j, :], kernel_type,␣ 􏰣→kernel_params)\n","  # initialize the support vectors\n","  a_g = np.zeros(shape=(M, )) #initialize the threshold\n","  b_g = 0\n","  #u contains the values of the sv expansion for each input\n","  u_g = np.zeros(shape=(M, ))\n","  # KKT(i) is 1 if the i-th sv (a(i)) satisfies the KKT conditions # KKT(i) is 0 otherwise\n","  KKT_g=np.zeros(shape=(M, ))\n","  # Update KKT conditions\n","  for k in range(0, M):\n","  r2 = (u_g[k]-y[k])*y[k]\n","  if ((r2 < -tol) and (a_g[k] < C)) or ((r2 > tol) and (a_g[k] > 0) ):\n","  KKT_g[k] = 0 # KKT condition not satisfied else:\n","  KKT_g[k] = 1\n","  # NB(i) is 1 if the sv a(i) is non bound, i.e. 0<a(i)<C # NB(i) is 0 otherwise.\n","  NB_g = np.zeros(shape=(M, ))\n","  numChanged=0\n","  examineAll=1\n","  while (numChanged > 0) or (examineAll == 1):\n","  numChanged=0\n","  if examineAll==1:\n","  # loop over all training examples\n","  for i in range(0, M):\n","  numChanged = numChanged + examineExample(i, x, y, C, epsilon,␣\n","  􏰣→Kernel_matrix)\n","  else:\n","  # loop over all training examples, where a(i) is non-bound for i in range(0, M):\n","  if (NB_g[i] == 1):\n","  numChanged = numChanged + examineExample(i, x, y, C,␣\n","  􏰣→epsilon, Kernel_matrix)\n","  if examineAll == 1: examineAll = 0\n","  3\n","\n","  elif numChanged == 0: examineAll = 1\n","  a = np.array(a_g)\n","  b = np.array(b_g)\n","  print('SMO Finished')\n","  return a, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wXM_iajyegiI"},"source":["(a) Plot the points $[x_{n,1},x_{n,2}]$ using different colors for each class."]},{"cell_type":"code","metadata":{"id":"SV6d9iATervU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yD-59dVLesL3"},"source":["(b) Use SVM with the Gaussian kernel for $\\sigma$ = 20 and set $C$ = 1. Plot the classifier and the margin. Moreover, find the support vectors (i.e., the points with nonzero Lagrange multipliers that contribute to the expansion of the classifier) and plot them as circled points."]},{"cell_type":"code","metadata":{"id":"--LzOlI9ey9u"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWapyWTAezMC"},"source":["(c) Repeat step (b) using $C$ = 0.5, 0.1, 0.05."]},{"cell_type":"code","metadata":{"id":"nKnC8y-Mez21"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HtKighEsez87"},"source":["(d) Repeat step (b) using $C$ = 5, 10, 50, 100."]},{"cell_type":"code","metadata":{"id":"b64hKQ-Fe4JB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WUVqlL1he4Pm"},"source":["(e) Comment on the results."]},{"cell_type":"markdown","metadata":{"id":"qWZoyFcw971A"},"source":["# Output"]},{"cell_type":"code","metadata":{"id":"Zh6y-Cbx986W"},"source":["# should access the Google Drive files before running the chunk\n","%%capture\n","!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic \n","!jupyter nbconvert --to pdf \"/content/drive/MyDrive/American_University/2021_Fall/DATA-642-001_Advanced Machine Learning/GitHub/Labs/07/submit/Lab7_Yunting.ipynb\""],"execution_count":null,"outputs":[]}]}