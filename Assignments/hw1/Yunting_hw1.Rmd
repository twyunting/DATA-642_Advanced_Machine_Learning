---
title: "Advanced Machine Learning - HW1"
author: "Yunting Chiu"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![](pics/ex1.png)

If $A$ is a  real n-by-n symmetric matrix, then $A$ = $A^T$.\
Let $\lambda$ be a (real) eigenvalue of $A$ and $V$ be the corresponding real eigenvector. Now, we have:
$$
A V = \lambda V \tag{1.1}
$$

Then we multiply by $V^T$ on both sides ($V^T$ is a transpose of $V$), the following equation would be:\
Note: The outcome of $V^T * V$ would be 1-by-1 vector.
$$
V^TAV = \lambda V^T V  = \lambda ||V||^2\tag{1.2}
$$

Because $A$ is positive definite so the left hand side is positive, and $V$ is a non-zero eigenvector. Also, the length $||V||^2$ must be a positive, we can derive the eigenvalues of a symmetric positive matrix $\lambda$ is positive. It follows that every eigenvalue $\lambda$ of $A$ is real.
$$
\lambda = \frac{V^TAV}{||V||^2} > 0\tag{1.3}
$$

![](pics/ex2.png)
**Prove A:**

- Let $I_n$ denotes a n-by-n identity matrix, and A a n-by-n orthogonal matrix. According to the definition of an orthogonal matrix, we know that: $AA^{T}=I=A^{T}A$.
- Also, we reviewed this from the class: $det(A \cdot B) = det(A) \cdot det(B)$ and $det(A) = det(A^T)$.
- Now we have $\det(I) = 1 = \det(AA^T) = \det(A) \det(A^T) = \det(A) \det(A) = [\det(A)]^2$
- $[\det(A)]^2$ so $\det(A) = \sqrt{1} = \pm 1$

**Prove R:**

By definition, a rotation matrix in n-dimensions is a n-by-n special orthogonal matrix. Therefore, it is a subset of the orthogonal group. Following the same proof above, we can know that the determinant of a rotation matrix is $\pm 1$.

![](pics/ex3.png)

We know that $\lambda$ is a eigenvalue of $A$, and $V$ is a corresponding real eigenvector. To be precise, $A$ is a n-by-n matrix, $V$ is a non-zero n-by-1 vector, and $\lambda$ is a scalar (real or complex number).

$$ 
A V = \lambda V \tag{3.1}
$$

Let's use $A^TA$ be a matrix as a whole to replace $A$, the equation would be:
$$
A^T A V = \lambda V \tag{3.2}
$$

Then, we multiply by $A$ on both sides, the equation would be:

$$
A A^T A V = \lambda A V \tag{3.3}
$$

Because $A$ is a n-by-n matrix, $V$ is a non-zero n-by-1 vector so the result of $A*V$ is a n-by-1 value. In equation 3.2, $A^T A V$ is equal to $\lambda V$; In equation 3.3, $A A^T A V$ is equal to $\lambda A V$. Therefore, we can find out there is a same eigenvalue $\lambda$ between $A^T A$ and $A A^T$.


![](pics/ex4.png)

The Hessian matrix, also known as the Hessian, is a square matrix of **second-order** partial derivatives of a scalar-valued function, or scalar field. 

The negative entropy function is:
$$
f(x) = -x\space\ log\space x\tag{4.1}
$$

We take the first derivative:
$$
f'(x) = -1 + log(x)\tag{4.2}
$$

We take the second derivative:
$$
f''(x) = \frac{1}{x}\tag{4.3}
$$

Because $x>0$, then $\frac{1}{x} > 0$. The second-order condition can only be used for twice-differentiable functions. If $\nabla_{\boldsymbol{x}}^2f(\boldsymbol{x})>\mathbf{0}$, we can conclude that it is strictly convex. 

![](pics/ex5.png)

Where $A \in R^{k \times n}$, $b \in R^{k}$, $x \in R^{n}$, and $||\cdot||_2$ denotes the Euclidean norm.

For the least-square objective function, we take the first derivative:
$$
\nabla f(x) = 2A^{T}(Ax - b)\tag{5.1}
$$

Then, we take the second derivative, and it would always convex.
$$
\nabla^{2}f(x) = 2A^{T}A\tag{5.2}
$$

The Hessian is $A^TA$, which is positive semi-definite. It follows that $f$ is strict convexity.

![](pics/ex6.png)

The norm $||A||_2$ is always called the *spectral norm* so the spectral norm of a matrix $A \in R^{m \times n}$ is denoted as 
$$
||A||_2  =  \max\limits_{x \neq0} \frac{||Ax||_2}{||x||_2} = \max\limits_{u \in R^{n} : ||u||_2 = 1} ||Au||_2 \tag{6.1}
$$


To proof, we first square the above equation:
$$
||A||^2_2 = \max\limits_{x \neq0} \frac{||Ax||^2_2}{||x||^2_2} = \max\limits_{x \neq0} \frac{x^{T}A^{T}Ax}{x^{T}x} = \sqrt{\lambda max(A^{T} A)} = \lambda_1 (A^{T} A) = \sigma_1 (A)^2\tag{6.2}
$$
Where $\lambda$ is the eigenvalues of $(A^{T} A)$ so we have 
$$
\left \| A \right \| _2=\sqrt{\lambda_{\text{max}}(A^{^*}A)}=\sigma_{\text{max}}(A)\tag{6.3}
$$

Because singular value is defined as the square roots of non-negative eigenvalues of the self-adjoint operator $A^{T} A$.

# References
- https://yutsumura.com/positive-definite-real-symmetric-matrix-and-its-eigenvalues/
- https://lpsa.swarthmore.edu/MtrxVibe/EigMat/MatrixEigen.html
- https://www.wikihow.com/Find-Eigenvalues-and-Eigenvectors
- https://www.brainm.com/software/pubs/math/Rotation_matrix.pdf
- https://math.stackexchange.com/questions/483339/proof-of-convexity-of-linear-least-squares
- http://web.cs.ucla.edu/~patricia.xiao/files/ECE_236B_Course_Notes.pdf
- https://www.cis.upenn.edu/~cis515/cis515-12-sl4.pdf
- https://math.stackexchange.com/questions/586663/why-does-the-spectral-norm-equal-the-largest-singular-value
- https://math.stackexchange.com/questions/586663/why-does-the-spectral-norm-equal-the-largest-singular-value
- https://en.wikipedia.org/wiki/Singular_value

